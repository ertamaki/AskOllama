% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AskOllama.R
\name{ask_ollama}
\alias{ask_ollama}
\title{Query Ollama's Chat Completion Endpoint}
\usage{
ask_ollama(
  messages = NULL,
  system_content = "You are a helpful assistant.",
  user_content = NULL,
  model = "mistral-large:123b-instruct-2411-q8_0",
  temperature = 0.8,
  num_predict = 500,
  top_p = 0.9,
  top_k = 40,
  repeat_penalty = 1.1,
  presence_penalty = 0,
  frequency_penalty = 0,
  stop = NULL,
  stream = FALSE,
  seed = NULL,
  strict_token_limit = FALSE,
  web = FALSE,
  web_url = NULL,
  web_content_limit = 20000,
  verbose = FALSE
)
}
\arguments{
\item{system_content}{Character string containing system instructions}

\item{user_content}{Character string containing user message}

\item{model}{Character string specifying the Ollama model name}

\item{temperature}{Numeric between 0 and 1 controlling randomness}

\item{num_predict}{Integer specifying how many tokens to generate}

\item{top_p}{Numeric between 0 and 1 for nucleus sampling}

\item{top_k}{Integer for top-k sampling}

\item{repeat_penalty}{Numeric penalty for token repetition}

\item{presence_penalty}{Numeric penalty for token presence}

\item{frequency_penalty}{Numeric penalty for token frequency}

\item{stop}{Character vector of strings where the model should stop generating}

\item{stream}{Logical. If \code{TRUE}, responses are streamed back incrementally}

\item{seed}{Integer for reproducibility}

\item{strict_token_limit}{Logical. If \code{TRUE}, forces strict token limiting (uses streaming)}

\item{web}{Logical. If \code{TRUE}, enables web research.}

\item{web_url}{URL to fetch web content from.}

\item{web_content_limit}{Maximum number of characters to be fetched.}

\item{verbose}{Logical. If \code{TRUE}, shows detailed request/response information and displays progress bars.}
}
\value{
Character string containing the model's response
}
\description{
This function provides a versatile interface to query Ollama's Chat Completion endpoint.
It supports single-turn, few-shot, and multi-turn conversational prompts and can incorporate
web-based research to enhance the prompt context. Users can customize various parameters to
control the response generation.
}
\examples{
\dontrun{
  # A simple query
  response <- ask_ollama(
    system_content = "You are a helpful assistant.",
    user_content = "Tell me a short story",
    model = "mistral-large:123b-instruct-2411-q8_0"
  )
  print(response)
}
}
